{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e0f5fa2",
   "metadata": {},
   "source": [
    "# Homework 10 - Self-supervised Learning\n",
    "\n",
    "Self-supervised (SSL) learning is a type of machine learning algorithms that use unlabeled data to learn representations of the data. \n",
    "It is different from supervised learning in that it does not require labeled data - instead, the algorithm learns by exploring the data and trying to uncover underlying patterns. \n",
    "This allows for models to learn more complex representations of the data and make better predictions.\n",
    "\n",
    "### [SSL methods]((https://arxiv.org/pdf/2006.08218.pdf)):\n",
    "- Reconstruct from a corrupted (or partial) version\n",
    "  - [Denoising Autoencoders](https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf), [Masked Autoencoders](https://arxiv.org/abs/2111.06377)\n",
    "  - [In-painting](https://arxiv.org/abs/1604.07379)\n",
    "- Visual common sense tasks\n",
    "  - [Relative patch prediction](https://arxiv.org/abs/1505.05192)\n",
    "  - [Jigsaw puzzles](https://arxiv.org/abs/1603.09246)\n",
    "  - [Rotation prediction](https://arxiv.org/abs/1803.07728)\n",
    "- Contrastive Learning\n",
    "  - [Simple Framework for Contrastive Learning of Visual Representations (SimCLR)](https://arxiv.org/abs/2002.05709)\n",
    "  - [Momentum Contrast (MoCo)](https://arxiv.org/abs/1911.05722)\n",
    "  - [Bootstrap Your Own Latent (BYOL)](https://arxiv.org/abs/2006.07733)\n",
    "\n",
    "The details of the method can be found by clicking on it, and implementations can be found in [OpenMixup](https://github.com/Westlake-AI/openmixup).\n",
    "\n",
    "In this notebook, we will start form implementing an simple Autoencoder:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1adeae4b",
   "metadata": {},
   "source": [
    "## Autoencoder\n",
    "\n",
    " Autoencoders (AEs) attempt to find a compressed representation for some dataset. The model does this by encoding the data into a smaller number of values, a compressed vector. During training, the vector is passed through a decoder which attempts to reconstruct the original data. The loss is then the mean squared error (MSE) between the original data $x_{ori}$ and reconstruction $x_{rec}$.\n",
    "\n",
    " $$\\ell_{MSE} = \\sum_{i=1}^{D}(x_{ori}-x_{rec})^2$$\n",
    "\n",
    "![A autoencoder. ](./imgs/AE.png)\n",
    "\n",
    "The compressed space is also named Latent Space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0ec280",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from utils import test_network, view_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "028f94d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data \n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5), (0.5)),\n",
    "                            ])\n",
    "trainset = datasets.MNIST('./data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = datasets.MNIST('./data/', train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20d83a13",
   "metadata": {},
   "source": [
    "## Network Building"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dae0bf5b",
   "metadata": {},
   "source": [
    "We will be constructing a network for an autoencoder by creating a class that is derived from the `nn.Module`. \n",
    "\n",
    "The approach I took was to split the forward pass into two parts which are the encoder and decoder. \n",
    "This is done so that after training we will mainly use the encoded vector. \n",
    "In the `forward` method, the connections between the two will be established.\n",
    "\n",
    "```python\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the layers and operations here\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        logits = self.decoder(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def encoder(self, x):\n",
    "        # Forward pass through the encoder\n",
    "        return x\n",
    "\n",
    "    def decoder(self, x):\n",
    "        # Forward pass through the decoder\n",
    "        return x\n",
    "```\n",
    "\n",
    "### Encoder\n",
    "To create the encoder, we'll combine nn.Conv2d with dropout from `nn.Dropout2d`.\n",
    "There is an option to downsample using max-pooling but many developers are opting to use strides instead. \n",
    "When inferring, you should turn off dropout and other optional layers that can be disabled like batch normalization with `net.eval()`. \n",
    "To reverse it, use `net.train()`.\n",
    "\n",
    "### Decoder\n",
    "With the decoder, you'll need to upsample the layers. This can be done with transposed convolutions (`nn.ConvTranspose2d`) or by nearest neighbor upsampling (`nn.UpsamplingNearest2d`). With transposed convolutions, you define the kernel size and strides like normal, but when you call the module, you can set an output size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "393d1891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, drop_prob=0.5):\n",
    "        super().__init__()\n",
    "        # Tip: Encoder\n",
    "        pass\n",
    "        \n",
    "        # Tip: Decoder\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encode(x)\n",
    "        logits = self.decode(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    def encode(self, x):\n",
    "        # Tip: encoding x to the latent z\n",
    "        pass\n",
    "        \n",
    "    def decode(self, x):\n",
    "        # Tip: decoding the latent z to x'\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a107bc7",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now we'll train the network. We're using the MSE for the loss, so we'll use `criterion = nn.MSELoss()`. As before, we update the weights by doing a forward pass through the network, then calculate the loss, get the gradients with `loss.backward()`, then make an update step with `optimizer.step()` (with an Adam optimizer, optim.Adam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5792358",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Autoencoder(drop_prob=0.1)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "epochs = 10\n",
    "print_every = 200  # Number of training steps to print losses\n",
    "show_every = 500  # Number of training steps to show reconstructions\n",
    "cuda = True        # Train on GPU or not\n",
    "\n",
    "if cuda:\n",
    "    net.cuda()\n",
    "\n",
    "running_loss = 0\n",
    "for e in range(epochs):\n",
    "    start = time.time() # Start timing\n",
    "    for i, (images, _) in enumerate(trainloader, 0): # Grab the images and labels\n",
    "        \n",
    "        inputs = Variable(images)\n",
    "        targets = Variable(images) # self-supervised\n",
    "\n",
    "        if cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        \n",
    "        optimizer.zero_grad() # Reset current gradients\n",
    "        \n",
    "        output = net.forward(inputs)\n",
    "        loss = criterion(output, targets) # Calculate the loss\n",
    "        loss.backward()\n",
    "        optimizer.step() # Update network weights\n",
    "        \n",
    "        running_loss += loss.item() # Accumulate the loss over all batches\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            net.eval()\n",
    "            stop = time.time()\n",
    "            # Test accuracy\n",
    "            val_loss = 0\n",
    "            for ii, (images, labels) in enumerate(testloader):\n",
    "            \n",
    "                inputs = Variable(images)\n",
    "                targets = Variable(images)\n",
    "                    \n",
    "                if cuda:\n",
    "                    inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                \n",
    "                output = net.forward(inputs)\n",
    "                val_loss += criterion(output, targets).item()\n",
    "                \n",
    "            print(\"Epoch: {}/{}..\".format(e+1, epochs),\n",
    "                \"Loss: {:.4f}..\".format(running_loss/print_every),\n",
    "                \"Test loss: {:.4f}..\".format(val_loss/(ii+1)),\n",
    "                \"{:.4f} s/batch\".format((stop - start)/print_every)\n",
    "                )\n",
    "            \n",
    "            running_loss = 0\n",
    "            start = time.time()\n",
    "            net.train()\n",
    "            \n",
    "        if i % show_every == 0:\n",
    "            net.cpu()\n",
    "            net.eval()\n",
    "            img = images[3]\n",
    "            with torch.no_grad():\n",
    "                x = Variable(img.resize_(1, *img.size()))\n",
    "            recon = net(x)\n",
    "            \n",
    "            view_recon(img, recon)\n",
    "            plt.show()\n",
    "            if cuda:\n",
    "                net.cuda()\n",
    "            net.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "917b7ec3c84293b92ddb6ccd238cf6b108767bfe42396bb9d40df41deeda18cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
