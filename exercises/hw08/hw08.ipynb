{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6374a79",
   "metadata": {},
   "source": [
    "## Homework 8 - Training a GCN model on the Cora dataset\n",
    "\n",
    "Please implement the following three functions:\n",
    "- normalize_adj() - Normalize adjacent matrix\n",
    "- GraphConvolution() - Design a graph convolution layer\n",
    "- GCN() - Construct a graph convolutional Network\n",
    "\n",
    "Please train a two-layer GCN with hidden dimension of 32 on the Cora dataset and print the training results for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "299e76af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_508787/2454436645.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  adj_tensor = torch.tensor(adj)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "from utils import load_data, accuracy\n",
    "\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Normalize adjacency matrix.\"\"\"\n",
    "    # pass\n",
    "    # Calculate the degree matrix\n",
    "    degree = np.array(adj.sum(1))\n",
    "    degree_inv_sqrt = np.power(degree, -0.5).flatten()\n",
    "    degree_inv_sqrt[np.isinf(degree_inv_sqrt)] = 0.  # Handle division by zero\n",
    "    degree_inv_sqrt_matrix = np.diag(degree_inv_sqrt)\n",
    "\n",
    "  # Convert NumPy arrays to PyTorch tensors\n",
    "    adj_tensor = torch.tensor(adj)\n",
    "    degree_inv_sqrt_tensor = torch.tensor(degree_inv_sqrt_matrix)\n",
    "\n",
    "    # Normalize adjacency matrix\n",
    "    normalized_adj_tensor = adj_tensor.mm(degree_inv_sqrt_tensor).t().mm(degree_inv_sqrt_tensor)\n",
    "\n",
    "    return normalized_adj_tensor\n",
    "\n",
    "# Seed radom seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load dat\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
    "adj = normalize_adj(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75ce1e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "    \"\"\"Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        # Tip\n",
    "        # pass\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        # Tip\n",
    "        # pass\n",
    "        support = torch.mm(x, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias\n",
    "        return output\n",
    "\n",
    "        \n",
    "class GCN(nn.Module):\n",
    "    '''GCN model with two GCN layers'''\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        # Tip\n",
    "        # pass\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        # Tip\n",
    "        # pass\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = self.dropout(x)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    '''Train the model on the training data'''\n",
    "    \n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the val data\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch + 1),\n",
    "        'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "        'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "        'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "        'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "        'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "    \n",
    "def test():\n",
    "    '''Evaluate the model on the testing data'''\n",
    "    \n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\", \"loss= {:.4f}\".format(loss_test.item()), \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75e2503d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.8859 acc_train: 0.2857 loss_val: 1.8185 acc_val: 0.3500 time: 0.5230s\n",
      "Epoch: 0002 loss_train: 1.8213 acc_train: 0.2929 loss_val: 1.7876 acc_val: 0.3500 time: 0.0025s\n",
      "Epoch: 0003 loss_train: 1.7685 acc_train: 0.3000 loss_val: 1.7714 acc_val: 0.3500 time: 0.0014s\n",
      "Epoch: 0004 loss_train: 1.7298 acc_train: 0.3429 loss_val: 1.7463 acc_val: 0.4167 time: 0.0013s\n",
      "Epoch: 0005 loss_train: 1.6763 acc_train: 0.4214 loss_val: 1.7088 acc_val: 0.4367 time: 0.0013s\n",
      "Epoch: 0006 loss_train: 1.6324 acc_train: 0.4429 loss_val: 1.6676 acc_val: 0.4533 time: 0.0013s\n",
      "Epoch: 0007 loss_train: 1.5858 acc_train: 0.4714 loss_val: 1.6227 acc_val: 0.4833 time: 0.0013s\n",
      "Epoch: 0008 loss_train: 1.5134 acc_train: 0.5286 loss_val: 1.5744 acc_val: 0.5000 time: 0.0013s\n",
      "Epoch: 0009 loss_train: 1.4487 acc_train: 0.5500 loss_val: 1.5252 acc_val: 0.5067 time: 0.0015s\n",
      "Epoch: 0010 loss_train: 1.3891 acc_train: 0.5429 loss_val: 1.4753 acc_val: 0.4967 time: 0.0015s\n",
      "Epoch: 0011 loss_train: 1.2775 acc_train: 0.5857 loss_val: 1.4238 acc_val: 0.5300 time: 0.0015s\n",
      "Epoch: 0012 loss_train: 1.2279 acc_train: 0.6214 loss_val: 1.3684 acc_val: 0.5433 time: 0.0015s\n",
      "Epoch: 0013 loss_train: 1.1809 acc_train: 0.6286 loss_val: 1.3114 acc_val: 0.5833 time: 0.0015s\n",
      "Epoch: 0014 loss_train: 1.1011 acc_train: 0.6857 loss_val: 1.2562 acc_val: 0.6200 time: 0.0015s\n",
      "Epoch: 0015 loss_train: 1.0515 acc_train: 0.6929 loss_val: 1.2060 acc_val: 0.6733 time: 0.0015s\n",
      "Epoch: 0016 loss_train: 0.9542 acc_train: 0.7643 loss_val: 1.1587 acc_val: 0.6900 time: 0.0015s\n",
      "Epoch: 0017 loss_train: 0.9374 acc_train: 0.7714 loss_val: 1.1113 acc_val: 0.7100 time: 0.0015s\n",
      "Epoch: 0018 loss_train: 0.8375 acc_train: 0.8000 loss_val: 1.0658 acc_val: 0.7100 time: 0.0015s\n",
      "Epoch: 0019 loss_train: 0.8321 acc_train: 0.8571 loss_val: 1.0251 acc_val: 0.7067 time: 0.0015s\n",
      "Epoch: 0020 loss_train: 0.7741 acc_train: 0.8143 loss_val: 0.9888 acc_val: 0.7400 time: 0.0015s\n",
      "Epoch: 0021 loss_train: 0.7530 acc_train: 0.8286 loss_val: 0.9564 acc_val: 0.7567 time: 0.0015s\n",
      "Epoch: 0022 loss_train: 0.6886 acc_train: 0.8643 loss_val: 0.9280 acc_val: 0.7700 time: 0.0015s\n",
      "Epoch: 0023 loss_train: 0.7059 acc_train: 0.8786 loss_val: 0.9010 acc_val: 0.7700 time: 0.0015s\n",
      "Epoch: 0024 loss_train: 0.6488 acc_train: 0.8500 loss_val: 0.8748 acc_val: 0.7700 time: 0.0015s\n",
      "Epoch: 0025 loss_train: 0.5955 acc_train: 0.8643 loss_val: 0.8544 acc_val: 0.7733 time: 0.0015s\n",
      "Epoch: 0026 loss_train: 0.5763 acc_train: 0.9000 loss_val: 0.8414 acc_val: 0.7767 time: 0.0015s\n",
      "Epoch: 0027 loss_train: 0.5889 acc_train: 0.8500 loss_val: 0.8307 acc_val: 0.7900 time: 0.0015s\n",
      "Epoch: 0028 loss_train: 0.5295 acc_train: 0.9071 loss_val: 0.8136 acc_val: 0.8000 time: 0.0015s\n",
      "Epoch: 0029 loss_train: 0.5442 acc_train: 0.9071 loss_val: 0.7976 acc_val: 0.8000 time: 0.0015s\n",
      "Epoch: 0030 loss_train: 0.5651 acc_train: 0.8786 loss_val: 0.7869 acc_val: 0.7967 time: 0.0016s\n",
      "Epoch: 0031 loss_train: 0.4859 acc_train: 0.9143 loss_val: 0.7759 acc_val: 0.7933 time: 0.0016s\n",
      "Epoch: 0032 loss_train: 0.4924 acc_train: 0.9357 loss_val: 0.7682 acc_val: 0.8067 time: 0.0016s\n",
      "Epoch: 0033 loss_train: 0.4831 acc_train: 0.9071 loss_val: 0.7668 acc_val: 0.8033 time: 0.0016s\n",
      "Epoch: 0034 loss_train: 0.4626 acc_train: 0.9071 loss_val: 0.7585 acc_val: 0.8033 time: 0.0017s\n",
      "Epoch: 0035 loss_train: 0.4139 acc_train: 0.9500 loss_val: 0.7468 acc_val: 0.8000 time: 0.0016s\n",
      "Epoch: 0036 loss_train: 0.4237 acc_train: 0.9357 loss_val: 0.7382 acc_val: 0.8033 time: 0.0016s\n",
      "Epoch: 0037 loss_train: 0.4340 acc_train: 0.9286 loss_val: 0.7437 acc_val: 0.8167 time: 0.0016s\n",
      "Epoch: 0038 loss_train: 0.4482 acc_train: 0.9500 loss_val: 0.7323 acc_val: 0.8167 time: 0.0016s\n",
      "Epoch: 0039 loss_train: 0.4089 acc_train: 0.9286 loss_val: 0.7248 acc_val: 0.8133 time: 0.0017s\n",
      "Epoch: 0040 loss_train: 0.4219 acc_train: 0.9214 loss_val: 0.7244 acc_val: 0.8033 time: 0.0016s\n",
      "Epoch: 0041 loss_train: 0.4019 acc_train: 0.9429 loss_val: 0.7181 acc_val: 0.8000 time: 0.0016s\n",
      "Epoch: 0042 loss_train: 0.3978 acc_train: 0.9286 loss_val: 0.7155 acc_val: 0.8100 time: 0.0016s\n",
      "Epoch: 0043 loss_train: 0.4314 acc_train: 0.8786 loss_val: 0.7118 acc_val: 0.8067 time: 0.0016s\n",
      "Epoch: 0044 loss_train: 0.3669 acc_train: 0.9571 loss_val: 0.7044 acc_val: 0.8033 time: 0.0017s\n",
      "Epoch: 0045 loss_train: 0.3830 acc_train: 0.9429 loss_val: 0.6966 acc_val: 0.8100 time: 0.0016s\n",
      "Epoch: 0046 loss_train: 0.3285 acc_train: 0.9214 loss_val: 0.7011 acc_val: 0.8067 time: 0.0017s\n",
      "Epoch: 0047 loss_train: 0.3304 acc_train: 0.9429 loss_val: 0.6980 acc_val: 0.8100 time: 0.0016s\n",
      "Epoch: 0048 loss_train: 0.3379 acc_train: 0.9429 loss_val: 0.7078 acc_val: 0.8033 time: 0.0016s\n",
      "Epoch: 0049 loss_train: 0.3536 acc_train: 0.9286 loss_val: 0.7058 acc_val: 0.8067 time: 0.0016s\n",
      "Epoch: 0050 loss_train: 0.3521 acc_train: 0.9214 loss_val: 0.6895 acc_val: 0.8133 time: 0.0016s\n",
      "Epoch: 0051 loss_train: 0.3515 acc_train: 0.9571 loss_val: 0.6851 acc_val: 0.8100 time: 0.0017s\n",
      "Epoch: 0052 loss_train: 0.3939 acc_train: 0.9286 loss_val: 0.6909 acc_val: 0.7967 time: 0.0016s\n",
      "Epoch: 0053 loss_train: 0.3572 acc_train: 0.9500 loss_val: 0.7019 acc_val: 0.7867 time: 0.0016s\n",
      "Epoch: 0054 loss_train: 0.3310 acc_train: 0.9500 loss_val: 0.7071 acc_val: 0.8000 time: 0.0016s\n",
      "Epoch: 0055 loss_train: 0.3260 acc_train: 0.9429 loss_val: 0.7048 acc_val: 0.8067 time: 0.0016s\n",
      "Epoch: 0056 loss_train: 0.3416 acc_train: 0.9143 loss_val: 0.7001 acc_val: 0.8067 time: 0.0016s\n",
      "Epoch: 0057 loss_train: 0.3248 acc_train: 0.9429 loss_val: 0.6859 acc_val: 0.8067 time: 0.0016s\n",
      "Epoch: 0058 loss_train: 0.3407 acc_train: 0.9500 loss_val: 0.6743 acc_val: 0.8067 time: 0.0017s\n",
      "Epoch: 0059 loss_train: 0.3349 acc_train: 0.9286 loss_val: 0.6799 acc_val: 0.7967 time: 0.0016s\n",
      "Epoch: 0060 loss_train: 0.3414 acc_train: 0.9500 loss_val: 0.7083 acc_val: 0.7900 time: 0.0016s\n",
      "Epoch: 0061 loss_train: 0.3262 acc_train: 0.9500 loss_val: 0.6974 acc_val: 0.7900 time: 0.0016s\n",
      "Epoch: 0062 loss_train: 0.3088 acc_train: 0.9500 loss_val: 0.6782 acc_val: 0.8000 time: 0.0018s\n",
      "Epoch: 0063 loss_train: 0.3034 acc_train: 0.9429 loss_val: 0.6763 acc_val: 0.7900 time: 0.0017s\n",
      "Epoch: 0064 loss_train: 0.3141 acc_train: 0.9714 loss_val: 0.6763 acc_val: 0.8100 time: 0.0017s\n",
      "Epoch: 0065 loss_train: 0.3304 acc_train: 0.9357 loss_val: 0.6849 acc_val: 0.7967 time: 0.0017s\n",
      "Epoch: 0066 loss_train: 0.2920 acc_train: 0.9571 loss_val: 0.6843 acc_val: 0.8000 time: 0.0017s\n",
      "Epoch: 0067 loss_train: 0.2896 acc_train: 0.9429 loss_val: 0.6829 acc_val: 0.8033 time: 0.0017s\n",
      "Epoch: 0068 loss_train: 0.2811 acc_train: 0.9571 loss_val: 0.6708 acc_val: 0.8067 time: 0.0017s\n",
      "Epoch: 0069 loss_train: 0.2873 acc_train: 0.9643 loss_val: 0.6702 acc_val: 0.8100 time: 0.0017s\n",
      "Epoch: 0070 loss_train: 0.2894 acc_train: 0.9429 loss_val: 0.6722 acc_val: 0.8100 time: 0.0017s\n",
      "Epoch: 0071 loss_train: 0.2817 acc_train: 0.9500 loss_val: 0.6687 acc_val: 0.8133 time: 0.0017s\n",
      "Epoch: 0072 loss_train: 0.2808 acc_train: 0.9500 loss_val: 0.6689 acc_val: 0.8067 time: 0.0017s\n",
      "Epoch: 0073 loss_train: 0.3097 acc_train: 0.9286 loss_val: 0.6588 acc_val: 0.8133 time: 0.0017s\n",
      "Epoch: 0074 loss_train: 0.2691 acc_train: 0.9571 loss_val: 0.6664 acc_val: 0.8067 time: 0.0017s\n",
      "Epoch: 0075 loss_train: 0.2881 acc_train: 0.9429 loss_val: 0.6706 acc_val: 0.7967 time: 0.0017s\n",
      "Epoch: 0076 loss_train: 0.2504 acc_train: 0.9643 loss_val: 0.6798 acc_val: 0.8000 time: 0.0017s\n",
      "Epoch: 0077 loss_train: 0.2908 acc_train: 0.9571 loss_val: 0.6746 acc_val: 0.8000 time: 0.0017s\n",
      "Epoch: 0078 loss_train: 0.2939 acc_train: 0.9571 loss_val: 0.6541 acc_val: 0.8033 time: 0.0017s\n",
      "Epoch: 0079 loss_train: 0.2633 acc_train: 0.9643 loss_val: 0.6580 acc_val: 0.8200 time: 0.0017s\n",
      "Epoch: 0080 loss_train: 0.2919 acc_train: 0.9643 loss_val: 0.6625 acc_val: 0.8200 time: 0.0017s\n",
      "Epoch: 0081 loss_train: 0.2799 acc_train: 0.9571 loss_val: 0.6644 acc_val: 0.8067 time: 0.0017s\n",
      "Epoch: 0082 loss_train: 0.3043 acc_train: 0.9429 loss_val: 0.6806 acc_val: 0.8000 time: 0.0016s\n",
      "Epoch: 0083 loss_train: 0.2881 acc_train: 0.9571 loss_val: 0.6858 acc_val: 0.8033 time: 0.0016s\n",
      "Epoch: 0084 loss_train: 0.2772 acc_train: 0.9571 loss_val: 0.6677 acc_val: 0.7967 time: 0.0017s\n",
      "Epoch: 0085 loss_train: 0.2836 acc_train: 0.9643 loss_val: 0.6567 acc_val: 0.8167 time: 0.0017s\n",
      "Epoch: 0086 loss_train: 0.2946 acc_train: 0.9571 loss_val: 0.6537 acc_val: 0.8167 time: 0.0017s\n",
      "Epoch: 0087 loss_train: 0.2475 acc_train: 0.9714 loss_val: 0.6895 acc_val: 0.7967 time: 0.0017s\n",
      "Epoch: 0088 loss_train: 0.2881 acc_train: 0.9429 loss_val: 0.6681 acc_val: 0.7933 time: 0.0017s\n",
      "Epoch: 0089 loss_train: 0.2836 acc_train: 0.9500 loss_val: 0.6474 acc_val: 0.8033 time: 0.0017s\n",
      "Epoch: 0090 loss_train: 0.2714 acc_train: 0.9500 loss_val: 0.6629 acc_val: 0.7900 time: 0.0017s\n",
      "Epoch: 0091 loss_train: 0.2514 acc_train: 0.9714 loss_val: 0.6662 acc_val: 0.8033 time: 0.0017s\n",
      "Epoch: 0092 loss_train: 0.2628 acc_train: 0.9643 loss_val: 0.6726 acc_val: 0.8067 time: 0.0017s\n",
      "Epoch: 0093 loss_train: 0.2567 acc_train: 0.9643 loss_val: 0.6801 acc_val: 0.8067 time: 0.0017s\n",
      "Epoch: 0094 loss_train: 0.2522 acc_train: 0.9571 loss_val: 0.6670 acc_val: 0.8200 time: 0.0017s\n",
      "Epoch: 0095 loss_train: 0.2496 acc_train: 0.9786 loss_val: 0.6581 acc_val: 0.8200 time: 0.0017s\n",
      "Epoch: 0096 loss_train: 0.2603 acc_train: 0.9357 loss_val: 0.6522 acc_val: 0.8100 time: 0.0017s\n",
      "Epoch: 0097 loss_train: 0.2718 acc_train: 0.9643 loss_val: 0.6558 acc_val: 0.7967 time: 0.0017s\n",
      "Epoch: 0098 loss_train: 0.2742 acc_train: 0.9571 loss_val: 0.6651 acc_val: 0.7867 time: 0.0017s\n",
      "Epoch: 0099 loss_train: 0.2598 acc_train: 0.9571 loss_val: 0.6538 acc_val: 0.7933 time: 0.0017s\n",
      "Epoch: 0100 loss_train: 0.2732 acc_train: 0.9571 loss_val: 0.6571 acc_val: 0.8100 time: 0.0017s\n",
      "Epoch: 0101 loss_train: 0.3025 acc_train: 0.9429 loss_val: 0.6598 acc_val: 0.8233 time: 0.0017s\n",
      "Epoch: 0102 loss_train: 0.2803 acc_train: 0.9500 loss_val: 0.6432 acc_val: 0.8267 time: 0.0017s\n",
      "Epoch: 0103 loss_train: 0.2870 acc_train: 0.9571 loss_val: 0.6388 acc_val: 0.8200 time: 0.0017s\n",
      "Epoch: 0104 loss_train: 0.3003 acc_train: 0.9500 loss_val: 0.6423 acc_val: 0.8000 time: 0.0016s\n",
      "Epoch: 0105 loss_train: 0.2749 acc_train: 0.9643 loss_val: 0.6648 acc_val: 0.8033 time: 0.0017s\n",
      "Epoch: 0106 loss_train: 0.2963 acc_train: 0.9500 loss_val: 0.6815 acc_val: 0.7967 time: 0.0017s\n",
      "Epoch: 0107 loss_train: 0.2550 acc_train: 0.9571 loss_val: 0.6723 acc_val: 0.8000 time: 0.0017s\n",
      "Epoch: 0108 loss_train: 0.2312 acc_train: 0.9786 loss_val: 0.6517 acc_val: 0.8100 time: 0.0017s\n",
      "Epoch: 0109 loss_train: 0.2139 acc_train: 0.9786 loss_val: 0.6496 acc_val: 0.8133 time: 0.0017s\n",
      "Epoch: 0110 loss_train: 0.2429 acc_train: 0.9857 loss_val: 0.6514 acc_val: 0.8033 time: 0.0017s\n",
      "Epoch: 0111 loss_train: 0.2605 acc_train: 0.9571 loss_val: 0.6740 acc_val: 0.7933 time: 0.0016s\n",
      "Epoch: 0112 loss_train: 0.2602 acc_train: 0.9714 loss_val: 0.6805 acc_val: 0.7933 time: 0.0017s\n",
      "Epoch: 0113 loss_train: 0.2571 acc_train: 0.9643 loss_val: 0.6666 acc_val: 0.8000 time: 0.0017s\n",
      "Epoch: 0114 loss_train: 0.2685 acc_train: 0.9500 loss_val: 0.6590 acc_val: 0.8167 time: 0.0017s\n",
      "Epoch: 0115 loss_train: 0.2790 acc_train: 0.9571 loss_val: 0.6541 acc_val: 0.8200 time: 0.0017s\n",
      "Epoch: 0116 loss_train: 0.2897 acc_train: 0.9357 loss_val: 0.6497 acc_val: 0.8133 time: 0.0016s\n",
      "Epoch: 0117 loss_train: 0.2553 acc_train: 0.9643 loss_val: 0.6637 acc_val: 0.8000 time: 0.0017s\n",
      "Epoch: 0118 loss_train: 0.2489 acc_train: 0.9714 loss_val: 0.6648 acc_val: 0.8100 time: 0.0016s\n",
      "Epoch: 0119 loss_train: 0.2813 acc_train: 0.9571 loss_val: 0.6494 acc_val: 0.8167 time: 0.0017s\n",
      "Epoch: 0120 loss_train: 0.2632 acc_train: 0.9857 loss_val: 0.6410 acc_val: 0.8300 time: 0.0017s\n",
      "Epoch: 0121 loss_train: 0.2839 acc_train: 0.9786 loss_val: 0.6402 acc_val: 0.8100 time: 0.0017s\n",
      "Epoch: 0122 loss_train: 0.2730 acc_train: 0.9571 loss_val: 0.6434 acc_val: 0.8133 time: 0.0017s\n",
      "Epoch: 0123 loss_train: 0.2628 acc_train: 0.9643 loss_val: 0.6684 acc_val: 0.8000 time: 0.0029s\n",
      "Epoch: 0124 loss_train: 0.2381 acc_train: 0.9786 loss_val: 0.6690 acc_val: 0.8033 time: 0.0019s\n",
      "Epoch: 0125 loss_train: 0.2499 acc_train: 0.9786 loss_val: 0.6526 acc_val: 0.8100 time: 0.0017s\n",
      "Epoch: 0126 loss_train: 0.2450 acc_train: 0.9500 loss_val: 0.6482 acc_val: 0.8000 time: 0.0017s\n",
      "Epoch: 0127 loss_train: 0.2690 acc_train: 0.9786 loss_val: 0.6475 acc_val: 0.8033 time: 0.0017s\n",
      "Epoch: 0128 loss_train: 0.2521 acc_train: 0.9643 loss_val: 0.6418 acc_val: 0.8133 time: 0.0017s\n",
      "Epoch: 0129 loss_train: 0.2627 acc_train: 0.9571 loss_val: 0.6577 acc_val: 0.7967 time: 0.0017s\n",
      "Epoch: 0130 loss_train: 0.2444 acc_train: 0.9571 loss_val: 0.6680 acc_val: 0.7833 time: 0.0017s\n",
      "Epoch: 0131 loss_train: 0.2516 acc_train: 0.9500 loss_val: 0.6615 acc_val: 0.7933 time: 0.0017s\n",
      "Epoch: 0132 loss_train: 0.2559 acc_train: 0.9500 loss_val: 0.6534 acc_val: 0.7933 time: 0.0017s\n",
      "Epoch: 0133 loss_train: 0.2428 acc_train: 0.9500 loss_val: 0.6507 acc_val: 0.7967 time: 0.0017s\n",
      "Epoch: 0134 loss_train: 0.2584 acc_train: 0.9714 loss_val: 0.6589 acc_val: 0.8100 time: 0.0017s\n",
      "Epoch: 0135 loss_train: 0.2320 acc_train: 0.9714 loss_val: 0.6769 acc_val: 0.7900 time: 0.0017s\n",
      "Epoch: 0136 loss_train: 0.2432 acc_train: 0.9714 loss_val: 0.6811 acc_val: 0.7900 time: 0.0017s\n",
      "Epoch: 0137 loss_train: 0.2512 acc_train: 0.9643 loss_val: 0.6650 acc_val: 0.8000 time: 0.0017s\n",
      "Epoch: 0138 loss_train: 0.2427 acc_train: 0.9571 loss_val: 0.6508 acc_val: 0.8033 time: 0.0017s\n",
      "Epoch: 0139 loss_train: 0.2388 acc_train: 0.9643 loss_val: 0.6531 acc_val: 0.8033 time: 0.0017s\n",
      "Epoch: 0140 loss_train: 0.2335 acc_train: 0.9571 loss_val: 0.6612 acc_val: 0.7900 time: 0.0017s\n",
      "Epoch: 0141 loss_train: 0.2669 acc_train: 0.9643 loss_val: 0.6666 acc_val: 0.7967 time: 0.0017s\n",
      "Epoch: 0142 loss_train: 0.2474 acc_train: 0.9429 loss_val: 0.6664 acc_val: 0.8067 time: 0.0017s\n",
      "Epoch: 0143 loss_train: 0.2577 acc_train: 0.9643 loss_val: 0.6826 acc_val: 0.7833 time: 0.0017s\n",
      "Epoch: 0144 loss_train: 0.2657 acc_train: 0.9643 loss_val: 0.6838 acc_val: 0.7867 time: 0.0017s\n",
      "Epoch: 0145 loss_train: 0.2790 acc_train: 0.9429 loss_val: 0.6607 acc_val: 0.8033 time: 0.0017s\n",
      "Epoch: 0146 loss_train: 0.2413 acc_train: 0.9786 loss_val: 0.6536 acc_val: 0.8133 time: 0.0017s\n",
      "Epoch: 0147 loss_train: 0.2338 acc_train: 0.9571 loss_val: 0.6656 acc_val: 0.8000 time: 0.0017s\n",
      "Epoch: 0148 loss_train: 0.2363 acc_train: 0.9786 loss_val: 0.6714 acc_val: 0.7933 time: 0.0017s\n",
      "Epoch: 0149 loss_train: 0.2652 acc_train: 0.9714 loss_val: 0.6604 acc_val: 0.8000 time: 0.0017s\n",
      "Epoch: 0150 loss_train: 0.2624 acc_train: 0.9500 loss_val: 0.6534 acc_val: 0.8067 time: 0.0017s\n",
      "Epoch: 0151 loss_train: 0.2308 acc_train: 0.9643 loss_val: 0.6592 acc_val: 0.8033 time: 0.0017s\n",
      "Epoch: 0152 loss_train: 0.2443 acc_train: 0.9857 loss_val: 0.6609 acc_val: 0.8033 time: 0.0017s\n",
      "Epoch: 0153 loss_train: 0.2400 acc_train: 0.9643 loss_val: 0.6546 acc_val: 0.8100 time: 0.0017s\n",
      "Epoch: 0154 loss_train: 0.2436 acc_train: 0.9643 loss_val: 0.6555 acc_val: 0.8033 time: 0.0017s\n",
      "Epoch: 0155 loss_train: 0.2380 acc_train: 0.9500 loss_val: 0.6565 acc_val: 0.7933 time: 0.0017s\n",
      "Epoch: 0156 loss_train: 0.2820 acc_train: 0.9714 loss_val: 0.6533 acc_val: 0.8000 time: 0.0017s\n",
      "Epoch: 0157 loss_train: 0.2480 acc_train: 0.9571 loss_val: 0.6592 acc_val: 0.7933 time: 0.0017s\n",
      "Epoch: 0158 loss_train: 0.2520 acc_train: 0.9786 loss_val: 0.6643 acc_val: 0.8033 time: 0.0017s\n",
      "Epoch: 0159 loss_train: 0.2309 acc_train: 0.9643 loss_val: 0.6661 acc_val: 0.8067 time: 0.0017s\n",
      "Epoch: 0160 loss_train: 0.2871 acc_train: 0.9500 loss_val: 0.6582 acc_val: 0.8100 time: 0.0017s\n",
      "Epoch: 0161 loss_train: 0.2481 acc_train: 0.9643 loss_val: 0.6541 acc_val: 0.8067 time: 0.0017s\n",
      "Epoch: 0162 loss_train: 0.2510 acc_train: 0.9500 loss_val: 0.6528 acc_val: 0.8000 time: 0.0017s\n",
      "Epoch: 0163 loss_train: 0.2294 acc_train: 0.9714 loss_val: 0.6548 acc_val: 0.8033 time: 0.0017s\n",
      "Epoch: 0164 loss_train: 0.2345 acc_train: 0.9786 loss_val: 0.6482 acc_val: 0.8033 time: 0.0017s\n",
      "Epoch: 0165 loss_train: 0.2099 acc_train: 0.9643 loss_val: 0.6441 acc_val: 0.8233 time: 0.0017s\n",
      "Epoch: 0166 loss_train: 0.2318 acc_train: 0.9571 loss_val: 0.6427 acc_val: 0.8133 time: 0.0017s\n",
      "Epoch: 0167 loss_train: 0.2697 acc_train: 0.9429 loss_val: 0.6520 acc_val: 0.8200 time: 0.0017s\n",
      "Epoch: 0168 loss_train: 0.2570 acc_train: 0.9643 loss_val: 0.6667 acc_val: 0.8067 time: 0.0017s\n",
      "Epoch: 0169 loss_train: 0.2444 acc_train: 0.9786 loss_val: 0.6575 acc_val: 0.8067 time: 0.0016s\n",
      "Epoch: 0170 loss_train: 0.2275 acc_train: 0.9714 loss_val: 0.6403 acc_val: 0.8100 time: 0.0017s\n",
      "Epoch: 0171 loss_train: 0.2202 acc_train: 0.9786 loss_val: 0.6390 acc_val: 0.8167 time: 0.0016s\n",
      "Epoch: 0172 loss_train: 0.2589 acc_train: 0.9429 loss_val: 0.6459 acc_val: 0.8167 time: 0.0016s\n",
      "Epoch: 0173 loss_train: 0.2534 acc_train: 0.9571 loss_val: 0.6501 acc_val: 0.8067 time: 0.0017s\n",
      "Epoch: 0174 loss_train: 0.2559 acc_train: 0.9571 loss_val: 0.6812 acc_val: 0.8000 time: 0.0016s\n",
      "Epoch: 0175 loss_train: 0.2951 acc_train: 0.9571 loss_val: 0.6826 acc_val: 0.7933 time: 0.0017s\n",
      "Epoch: 0176 loss_train: 0.2391 acc_train: 0.9714 loss_val: 0.6653 acc_val: 0.8033 time: 0.0016s\n",
      "Epoch: 0177 loss_train: 0.2470 acc_train: 0.9714 loss_val: 0.6565 acc_val: 0.8000 time: 0.0017s\n",
      "Epoch: 0178 loss_train: 0.2345 acc_train: 0.9714 loss_val: 0.6520 acc_val: 0.7900 time: 0.0016s\n",
      "Epoch: 0179 loss_train: 0.2369 acc_train: 0.9786 loss_val: 0.6528 acc_val: 0.7967 time: 0.0017s\n",
      "Epoch: 0180 loss_train: 0.2536 acc_train: 0.9714 loss_val: 0.6744 acc_val: 0.7967 time: 0.0017s\n",
      "Epoch: 0181 loss_train: 0.2843 acc_train: 0.9571 loss_val: 0.6717 acc_val: 0.8033 time: 0.0017s\n",
      "Epoch: 0182 loss_train: 0.2297 acc_train: 0.9643 loss_val: 0.6606 acc_val: 0.8067 time: 0.0017s\n",
      "Epoch: 0183 loss_train: 0.2343 acc_train: 0.9714 loss_val: 0.6685 acc_val: 0.8000 time: 0.0016s\n",
      "Epoch: 0184 loss_train: 0.2572 acc_train: 0.9571 loss_val: 0.6548 acc_val: 0.7967 time: 0.0017s\n",
      "Epoch: 0185 loss_train: 0.2500 acc_train: 0.9714 loss_val: 0.6442 acc_val: 0.8067 time: 0.0017s\n",
      "Epoch: 0186 loss_train: 0.2279 acc_train: 0.9714 loss_val: 0.6671 acc_val: 0.8067 time: 0.0017s\n",
      "Epoch: 0187 loss_train: 0.2286 acc_train: 0.9857 loss_val: 0.6623 acc_val: 0.8100 time: 0.0017s\n",
      "Epoch: 0188 loss_train: 0.2218 acc_train: 0.9643 loss_val: 0.6445 acc_val: 0.8067 time: 0.0017s\n",
      "Epoch: 0189 loss_train: 0.2485 acc_train: 0.9500 loss_val: 0.6376 acc_val: 0.8133 time: 0.0017s\n",
      "Epoch: 0190 loss_train: 0.2785 acc_train: 0.9429 loss_val: 0.6410 acc_val: 0.8167 time: 0.0017s\n",
      "Epoch: 0191 loss_train: 0.2565 acc_train: 0.9571 loss_val: 0.6497 acc_val: 0.8033 time: 0.0017s\n",
      "Epoch: 0192 loss_train: 0.2369 acc_train: 0.9643 loss_val: 0.6845 acc_val: 0.8067 time: 0.0017s\n",
      "Epoch: 0193 loss_train: 0.2465 acc_train: 0.9857 loss_val: 0.6813 acc_val: 0.8033 time: 0.0016s\n",
      "Epoch: 0194 loss_train: 0.2832 acc_train: 0.9643 loss_val: 0.6386 acc_val: 0.8100 time: 0.0017s\n",
      "Epoch: 0195 loss_train: 0.2488 acc_train: 0.9571 loss_val: 0.6398 acc_val: 0.8067 time: 0.0017s\n",
      "Epoch: 0196 loss_train: 0.2472 acc_train: 0.9643 loss_val: 0.6439 acc_val: 0.8033 time: 0.0017s\n",
      "Epoch: 0197 loss_train: 0.2694 acc_train: 0.9643 loss_val: 0.6490 acc_val: 0.8033 time: 0.0017s\n",
      "Epoch: 0198 loss_train: 0.2374 acc_train: 0.9786 loss_val: 0.6802 acc_val: 0.8033 time: 0.0017s\n",
      "Epoch: 0199 loss_train: 0.2338 acc_train: 0.9857 loss_val: 0.6891 acc_val: 0.8033 time: 0.0017s\n",
      "Epoch: 0200 loss_train: 0.2424 acc_train: 0.9643 loss_val: 0.6659 acc_val: 0.8133 time: 0.0016s\n",
      "Optimization Finished!\n",
      "Test set results: loss= 0.7015 accuracy= 0.8090\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epochs', type=int, default=200,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.05,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=32,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "args.cuda = torch.cuda.is_available()\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Model and optimizer\n",
    "model = GCN(nfeat=features.shape[1], nhid=args.hidden, nclass=labels.max().item() + 1, dropout=args.dropout)\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "# Load data into CUDA\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# Testing\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
