{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7 - Experiments on RNN and LSTM\n",
    "\n",
    "Please implement the following two functions:\n",
    "- MnistRNN() - Design a RNN\n",
    "- MnistLSTM() - Design a LSTM \n",
    "\n",
    "Please train two models on the Mnist dataset and print the training results for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose,ToTensor,Normalize\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "TEST_BATCH_SIZE = 1000\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "# dataloader for the dataset\n",
    "def get_dataloader(train,batch_size=BATCH_SIZE):\n",
    "    transform_fn = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize(mean = (0.1307,), std = (0.3081,))\n",
    "        ]) \n",
    "    dataset = MNIST(root = './data',train = train,transform = transform_fn, download = True)\n",
    "    data_loader = DataLoader(dataset,batch_size = batch_size,shuffle = True)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN\n",
    "class MnistRNN(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=128, output_dim=10):\n",
    "        super(MnistRNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                input_dim, hidden_dim,\n",
    "                kernel_size=4, stride=4),\n",
    "            nn.ReLU())\n",
    "\n",
    "        # Tip: define RNN\n",
    "        pass\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.stem(inputs)\n",
    "        inputs = inputs.view(inputs.size(0), self.hidden_dim, -1)  # B x 1 x (7x7)\n",
    "\n",
    "        # Tip: forward RNN\n",
    "        hidden = pass\n",
    "\n",
    "        output = self.softmax(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "class MnistLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=128, output_dim=10):\n",
    "        super(MnistLSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                input_dim, hidden_dim,\n",
    "                kernel_size=4, stride=4),\n",
    "            nn.ReLU())\n",
    "\n",
    "        # Tip: define LSTM (the official implementation in `nn`` can be used)\n",
    "        pass\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.stem(inputs)\n",
    "        inputs = inputs.view(inputs.size(0), -1, self.hidden_dim)  # B x (7x7) x D\n",
    "\n",
    "        # Tip: forward LSTM\n",
    "        output = pass\n",
    "\n",
    "        output = self.fc(output.mean([-2]))\n",
    "        output = self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MnistRNN().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, num_epochs):\n",
    "    data_loader = get_dataloader(True)\n",
    "    total_step = len(data_loader)\n",
    "    for idx, (input, target) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input.to(device))\n",
    "        loss = F.nll_loss(output, target.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (idx+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, idx+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    test_dataloader = get_dataloader(train = False,batch_size=TEST_BATCH_SIZE)\n",
    "    for idx,(input,target) in enumerate(test_dataloader):\n",
    "        with torch.no_grad():\n",
    "            output = model(input.to(device))\n",
    "            target = target.to(device)\n",
    "            cur_loss = F.nll_loss(output, target)\n",
    "            loss_list.append(cur_loss.cpu())\n",
    "            pred = output.max(dim = -1)[-1]\n",
    "            cur_acc = pred.eq(target).float().mean()\n",
    "            acc_list.append(cur_acc.cpu())\n",
    "    print(\"Mean accuracy: \", np.mean(acc_list), \"Mean loss: \", np.mean(loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()\n",
    "num_epochs = 3\n",
    "for i in range(num_epochs):\n",
    "    train(i, num_epochs)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MnistLSTM().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()\n",
    "num_epochs = 3\n",
    "for i in range(num_epochs):\n",
    "    train(i, num_epochs)\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
